{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091f5a09",
   "metadata": {},
   "source": [
    "question1:\n",
    "The theoretical Simple Linear Regression model is a way to describe the relationship between two variables: a predictor (independent variable) and an outcome (dependent variable). This model assumes a linear relationship, meaning that as the predictor variable changes, the outcome variable changes in a way that can be represented by a straight line.\n",
    "\n",
    "Components of the Model:\n",
    "Predictor Variable (X): This is the independent variable that we believe influences the outcome. It can be any measurable variable like time, age, temperature, etc.\n",
    "Outcome Variable (Y): This is the dependent variable, which we aim to predict or explain using the predictor variable. It might represent things like sales, test scores, or prices.\n",
    "Slope (β1): The slope indicates the rate at which the outcome variable Y changes with respect to changes in the predictor variable X. \n",
    "Intercept (β0): The intercept is the value of Y when X is zero. It represents the starting value of the outcome variable when there’s no influence from the predictor variable.\n",
    "Error Term (ε): The error term captures the randomness or unobserved influences that affect the outcome variable but aren’t explained by the predictor variable. This term accounts for the fact that the data points don’t lie perfectly on a straight line and introduces variability around the line.\n",
    "\n",
    "Summary:\n",
    "Explanation of Simple Linear Regression Model: I provided an overview of the key components in a simple linear regression model:\n",
    "Predictor Variable (X): The independent variable influencing the outcome.\n",
    "Outcome Variable (Y): The dependent variable we aim to predict.\n",
    "Slope (β1): Indicates how much Y changes with a unit increase in X.\n",
    "Intercept (β0): Represents the starting value of Y when X is zero.\n",
    "Error Term (ε): Captures random variation, assuming normal distribution, which causes Y to vary around the predicted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62afd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf  # This library provides functions to specify and fit statistical models using R-style formula syntax.\n",
    "import plotly.express as px  # This is a plotting library, particularly good for interactive visualizations.\n",
    "\n",
    "# Simulating the data from a theoretical Simple Linear Regression model\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)                     # Predictor variable\n",
    "error = np.random.normal(0, 1.5, size=x.shape)  # Random error term with standard deviation 1.5\n",
    "Y = 5 + 2 * x + error                           # Outcome variable using intercept=5 and slope=2\n",
    "\n",
    "# Creating a DataFrame to hold the simulated data\n",
    "df = pd.DataFrame({'x': x, 'Y': Y})\n",
    "\n",
    "# Model specification and fitting\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df)  # This specifies an Ordinary Least Squares (OLS) model with Y as the outcome and x as the predictor.\n",
    "fitted_model = model_data_specification.fit()         # Fits the model to the data.\n",
    "\n",
    "# Model summary and results\n",
    "print(fitted_model.summary())                         # Summary of the model fit, providing detailed statistics like coefficients, standard errors, t-values, and p-values.\n",
    "print(fitted_model.summary().tables[1])               # Shows a concise table of parameter estimates, standard errors, and related statistics.\n",
    "print(fitted_model.params)                            # Parameters (coefficients) for intercept and slope.\n",
    "print(fitted_model.params.values)                     # Parameter values as a list/array.\n",
    "print(fitted_model.rsquared)                          # R-squared, a measure of model fit, representing the proportion of variance in Y explained by x.\n",
    "\n",
    "# Visualization with plotly.express\n",
    "df['Data'] = 'Data'  # Hack to add data to legend \n",
    "fig = px.scatter(df, x='x', y='Y', color='Data', \n",
    "                 trendline='ols', title='Y vs. x')  # Adds a scatter plot of the data with an OLS trendline fitted to it.\n",
    "\n",
    "# Adding the OLS trendline manually for demonstration\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, \n",
    "                line=dict(color='blue'), name=\"trendline='ols'\")  # Adds a fitted trendline based on the model's predicted values.\n",
    "\n",
    "fig.show(renderer=\"png\")  # Use `renderer=\"png\"` for compatibility with GitHub and MarkUs submissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426741f",
   "metadata": {},
   "source": [
    "question2:\n",
    "Library Imports:\n",
    "import statsmodels.formula.api as smf: This library allows us to specify and fit statistical models in Python using R-style formulas. It’s used for defining regression models, including ordinary least squares (OLS) regression.\n",
    "import plotly.express as px: plotly.express is a powerful and interactive plotting library for Python, especially well-suited for creating and customizing data visualizations quickly.\n",
    "Model Specification and Fitting:\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df): This line specifies an OLS regression model where Y is the dependent variable and x is the independent variable. Here, \"Y ~ x\" represents the formula for a simple linear regression in the statsmodels framework.\n",
    "fitted_model = model_data_specification.fit(): This step fits the model to the data in df, calculating the optimal intercept and slope coefficients based on the least-squares criterion.\n",
    "Model Outputs:\n",
    "fitted_model.summary(): Provides a detailed summary of the fitted model, including key statistics such as the coefficient estimates, standard errors, t-values, p-values, and confidence intervals.\n",
    "fitted_model.summary().tables[1]: Shows the specific table containing parameter estimates for the intercept and slope, along with their standard errors and p-values, in a concise format.\n",
    "fitted_model.params: Displays the fitted model's coefficients, giving values for the intercept (β0) and slope (β1).\n",
    "fitted_model.params.values: Returns the model parameters (intercept and slope) as an array, making it easier to access them programmatically.\n",
    "fitted_model.rsquared: Provides the R-squared value, which is a measure of how well the model explains the variability of the outcome variable Y. Higher R-squared values indicate better model fit.\n",
    "Visualization with Plotly:\n",
    "df['Data'] = 'Data': This line creates a new column to label the scatter points in the plot legend as “Data”.\n",
    "fig = px.scatter(df, x='x', y='Y', color='Data', trendline='ols', title='Y vs. x'): Creates an interactive scatter plot of against with an OLS trendline fitted through the data.\n",
    "fig.add_scatter(...): Adds a manual trendline using the model’s fitted values. This demonstrates the trendline visually aligns with the one automatically added by trendline='ols'.\n",
    "This code provides a comprehensive way to simulate data, fit a simple linear regression model, inspect model parameters, and visualize the fitted model with Plotly.\n",
    "\n",
    "Summary:\n",
    "Goal: Demonstrate how to create and visualize a fitted Simple Linear Regression model using simulated data. The steps include specifying the model, fitting it, and visualizing the results.\n",
    "Explanation and Code:\n",
    "Model Simulation: We simulated data for X and Y based on a linear equation with added noise, and then created a DataFrame (df) containing these values.\n",
    "Library Imports:\n",
    "statsmodels.formula.api as smf: For specifying and fitting regression models with R-style formulas.\n",
    "plotly.express as px: For creating interactive visualizations.\n",
    "Model Specification & Fitting:\n",
    "Defined an OLS regression model (Y ~ x) with smf.ols.\n",
    "Fitted the model to get coefficients and model statistics.\n",
    "Model Outputs:\n",
    "Explained key model attributes, including summary(), coefficients (params), and the model fit (R-squared).\n",
    "Visualization:\n",
    "Used plotly.express to plot the data and add an OLS trendline, both automatically and manually.\n",
    "Inline Questions: Provided answers to each inline question about the purpose of libraries, functions, and attributes in the code, making it easier to understand each component.\n",
    "This summary covers the steps needed to create, fit, and visualize a Simple Linear Regression model using statsmodels and plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75214a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)                     # Predictor variable\n",
    "error = np.random.normal(0, 1.5, size=x.shape)  # Random error term with std deviation 1.5\n",
    "Y = 5 + 2 * x + error                           # Outcome variable with intercept=5 and slope=2\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'x': x, 'Y': Y})\n",
    "\n",
    "# Fit model\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df)\n",
    "fitted_model = model_data_specification.fit()\n",
    "\n",
    "# Visualization with plotly.express\n",
    "df['Data'] = 'Data'  # Add data label for legend\n",
    "fig = px.scatter(df, x='x', y='Y', color='Data', trendline='ols', title='Y vs. x with Fitted and True Regression Lines')\n",
    "\n",
    "# Add fitted trendline from model\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, line=dict(color='blue'), name=\"Fitted Trendline\")\n",
    "\n",
    "# Add true theoretical line from Question 1 (no error term, intercept=5 and slope=2)\n",
    "fig.add_scatter(x=x, y=5 + 2 * x, line=dict(color='red', dash=\"dash\"), name=\"True Regression Line\")\n",
    "\n",
    "fig.show(renderer=\"png\")  # Use renderer=\"png\" for compatibility with GitHub/MarkUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59fd61",
   "metadata": {},
   "source": [
    "question3:\n",
    "Explanation of the Difference Between the Two Lines:\n",
    "True Regression Line (red, dashed): This line represents the theoretical model we initially defined, with the intercept of 5 and slope of 2. It shows the ideal relationship between X andY without any random noise or sampling variation. This line is fixed and does not change.\n",
    "Fitted Trendline (blue, solid): This line is based on the actual sampled data that includes random error. Because of the noise in Y values, this line may have a slightly different intercept and slope from the theoretical line. This fitted line reflects the specific sample’s characteristics, showing how random variation can lead to a slight difference in the estimated relationship between X and Y.\n",
    "Key Insight: The difference between the two lines highlights the effect of random sampling variation. In real data, random noise means the fitted model may differ slightly from the true relationship due to this natural variability in observations.\n",
    "\n",
    "Summary:\n",
    "Objective: Add the theoretical line (true regression line) from the initial simulation to the plot with the fitted line from the regression model.\n",
    "Code Update: Added a red, dashed line for the true regression line based on original parameters (intercept = 5, slope = 2) without error. The fitted line (blue, solid) was added using the model's predicted values.\n",
    "Explanation of Difference:\n",
    "True Regression Line: Shows the ideal, underlying relationship between X and Y without random noise, representing the model’s theoretical values.\n",
    "Fitted Trendline: Reflects the sampled data with random variation, leading to slight deviations from the theoretical line.\n",
    "Key Insight: This difference illustrates the impact of random sampling variation. In real-world data, noise creates natural deviations between the fitted and true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6975a51",
   "metadata": {},
   "source": [
    "question4:\n",
    "The fitted_model.fittedvalues in a linear regression model are derived using the coefficients found in fitted_model.params (or specifically, the values in fitted_model.params.values). Here’s a step-by-step explanation of this process.\n",
    "Coefficients from the Model Summary:\n",
    "After fitting the model, fitted_model.summary().tables[1] (or simply fitted_model.params) provides the estimated coefficients:\n",
    "Intercept (β0): The constant term, which is the expected value of Y whenX is zero.\n",
    "Slope (β1): The rate at which Y changes for each one-unit increase in X.\n",
    "Using the Coefficients to Calculate Fitted Values:\n",
    "The fitted values (or predicted values), stored in fitted_model.fittedvalues, are calculated by plugging each X value from the dataset into this equation.\n",
    "This means each fitted_model.fittedvalues[i] is the computed prediction based on the intercept and slope from fitted_model.params.\n",
    "The fitted_model.fittedvalues array is created by applying the linear equation derived from the coefficients in fitted_model.params to each X value in the dataset. These fitted values represent the predicted Y values based on the best-fit line calculated by the model.\n",
    "Summary:\n",
    "Coefficients (fitted_model.params): After fitting the model, fitted_model.params provides the estimated intercept and slope values.\n",
    "Calculation of Fitted Values (fitted_model.fittedvalues):\n",
    "The fitted values are derived by plugging each X value from the dataset into the equation.\n",
    "This uses the intercept and slope from fitted_model.params to calculate predictions for each X value, resulting in fitted_model.fittedvalues.\n",
    "Result: fitted_model.fittedvalues represents the predicted Y values based on the model's best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68787a62",
   "metadata": {},
   "source": [
    "question5:\n",
    "The line chosen for the fitted model in Ordinary Least Squares (OLS) regression is the one that minimizes the sum of the squared differences between the observed Y values and the predicted Y^ values from the line.\n",
    "\n",
    "The reason OLS uses \"squares\" (i.e., it squares the differences) is to ensure that both positive and negative deviations are treated equally, avoiding cancellation that would occur if we summed the differences directly. Squaring also emphasizes larger errors, leading to a line that overall best fits the observed data by minimizing these squared discrepancies.\n",
    "Summary:\n",
    "In Ordinary Least Squares (OLS) regression, the chosen line minimizes the sum of squared differences between observed Y values and predicted Y^ values. Squaring these differences prevents positive and negative deviations from canceling each other out and gives more weight to larger errors, resulting in the best-fitting line for the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b860bf",
   "metadata": {},
   "source": [
    "qeustion6:\n",
    "Expression1:\n",
    "This expression is the formula for the coefficient of determination R^2 and it quantifies the proportion of the variation in Y that is explained by the model. \n",
    "\n",
    "Expression 2: fitted_model.rsquared\n",
    "\n",
    "Explanation: fitted_model.rsquared is simply the R^2 value of the fitted model. It is directly derived from the formula above and represents the proportion of variance in Y that is explained by the model. A higher R^2 value indicates that the model explains a larger portion of the variance in Y, and thus is a measure of the model's accuracy.\n",
    "\n",
    "Expression 3: np.corrcoef(Y, fitted_model.fittedvalues)[0, 1]**2\n",
    "\n",
    "Explanation: The correlation coefficient between Y and the fitted values Y^(i.e., fitted_model.fittedvalues) measures the strength and direction of the linear relationship between them. The square of this correlation coefficient, r^2, is the proportion of the variance in Y that can be explained by the model's predicted values.\n",
    "Interpretation: This expression captures the same information as R^2 because the square of the correlation between Y and Y^ represents the proportion of the variation in Y explained by the fitted model. It is another way of calculating how well the model fits the data.\n",
    "\n",
    "Expression 4: np.corrcoef(Y, x)[0, 1]**2\n",
    "\n",
    "Explanation: This is the square of the correlation coefficient between the independent variable X (or x in your code) and the dependent variable Y. It captures the strength of the linear relationship between X and Y. In the context of Simple Linear Regression, it gives us the proportion of the variance in Y that is explained by X.\n",
    "Interpretation: This expression is also equivalent to R^2 for the true relationship between X and Y. It represents the proportion of the variation in Y that is explained by X, irrespective of the model fitting. In other words, it tells us how strongly X explains the variation in Y before fitting the model.\n",
    "\n",
    "Summary:\n",
    "All these expressions measure how well the model or \n",
    "X explains the variation in Y, with R^2(and related calculations) serving as the key measure of model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2516ffd",
   "metadata": {},
   "source": [
    "question7:\n",
    "In Simple Linear Regression, the model makes several key assumptions that must hold for the results to be valid. Let's review a couple of assumptions that may not align with the example data you've provided.\n",
    "\n",
    "Assumptions of Simple Linear Regression:\n",
    "Linearity: There is a linear relationship between the predictor variable X (Amount of Fertilizer) and the outcome variable Y (Crop Yield).\n",
    "Independence: The residuals (the differences between observed and predicted values) are independent of each other.\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of X.\n",
    "Normality of residuals: The residuals are normally distributed.\n",
    "Possible Issues with the Example Data:\n",
    "Non-linearity:\n",
    "The data could potentially exhibit a non-linear relationship, especially since the crop yield seems to increase more rapidly at higher amounts of fertilizer (suggesting a potential curvilinear trend).\n",
    "This could violate the linearity assumption of Simple Linear Regression, which assumes that the relationship between X and Y is linear. The trendline might appear linear in the scatter plot, but the curvature could imply that a more complex model (e.g., polynomial regression) might be a better fit for the data.\n",
    "Heteroscedasticity:\n",
    "The residuals from the linear regression might show varying spread across the range ofX. For example, at higher levels of fertilizer, the residuals might increase in magnitude, suggesting that the variance of the residuals is not constant.\n",
    "This would violate the homoscedasticity assumption, which assumes that the variance of residuals is constant across all levels of the predictor variable. If the spread of residuals increases or decreases with X, this indicates heteroscedasticity, which can lead to unreliable parameter estimates.\n",
    "Checking These Assumptions:\n",
    "The scatter plot with the trendline can help assess the linearity assumption.\n",
    "The histogram of residuals can give insights into the normality of the residuals, but inspecting the residuals vs. fitted values plot would be more useful to assess homoscedasticity.\n",
    "\n",
    "Summary:\n",
    "Non-linearity might be an issue because of the potential curvilinear relationship between the fertilizer amount and crop yield.\n",
    "Heteroscedasticity could arise if the residuals' variance increases with fertilizer amount, violating the assumption of constant variance in residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question8\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the \"Classic\" Old Faithful Geyser dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Specify the linear regression model: duration ~ waiting\n",
    "linear_for_specification = 'duration ~ waiting'\n",
    "model = smf.ols(linear_for_specification, data=old_faithful)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# View the summary of the fitted model to get the p-value for the slope (beta_1)\n",
    "print(fitted_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72740dbc",
   "metadata": {},
   "source": [
    "qeustion8:\n",
    "Interpreting the Output\n",
    "In the output of fitted_model.summary(), we will find several pieces of important information, but the key value for hypothesis testing is the p-value for the slope (β1) under the coef table (the column labeled \"P>|t|\").\n",
    "Now, depending on the p-value, we will make a decision:\n",
    "\n",
    "If the p-value is less than 0.05, we reject the null hypothesis, meaning there is statistically significant evidence of a linear relationship between waiting time and eruption duration.\n",
    "If the p-value is greater than 0.05, we fail to reject the null hypothesis, meaning there is insufficient evidence to conclude a linear relationship between the waiting time and eruption duration.\n",
    "Interpreting the Evidence\n",
    "Here is how we interpret the evidence based on the p-value:\n",
    "\n",
    "Very Strong Evidence Against the Null Hypothesis: If the p-value is very small (e.g., less than 0.01), we have very strong evidence against the null hypothesis, indicating a clear linear association between the variables.\n",
    "Strong Evidence Against the Null Hypothesis: If the p-value is between 0.01 and 0.05, we have strong evidence against the null hypothesis.\n",
    "Moderate Evidence Against the Null Hypothesis: If the p-value is between 0.05 and 0.1, we have moderate evidence against the null hypothesis.\n",
    "Weak Evidence Against the Null Hypothesis: If the p-value is between 0.1 and 0.2, we have weak evidence against the null hypothesis.\n",
    "No Evidence Against the Null Hypothesis: If the p-value is greater than 0.2, we fail to reject the null hypothesis, meaning there is no sufficient evidence of a linear relationship.\n",
    "Example Interpretation\n",
    "Assuming the p-value for the slope β1 in the summary output is 0.002:\n",
    "\n",
    "Conclusion: \"We reject the null hypothesis with a p-value of 0.002, meaning we have strong evidence against the null hypothesis and conclude that there is a statistically significant linear association between the waiting time and eruption duration in the Old Faithful Geyser dataset.\"\n",
    "Alternatively, if the p-value is 0.15:\n",
    "\n",
    "Conclusion: \"We fail to reject the null hypothesis with a p-value of 0.15, meaning there is insufficient evidence to conclude a significant linear relationship between the waiting time and eruption duration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question9\n",
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set the limit for short wait times\n",
    "short_wait_limit = 62  # Change to 64 or 66 for other limits\n",
    "short_wait = old_faithful.waiting < short_wait_limit\n",
    "\n",
    "# Fit the model for short wait times\n",
    "model = smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit()\n",
    "\n",
    "# Print the summary of the fitted model\n",
    "print(model.summary().tables[1])\n",
    "\n",
    "# Create a scatter plot with the regression trendline\n",
    "fig = px.scatter(old_faithful[short_wait], x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions for short wait times (<\" + str(short_wait_limit) + \")\", \n",
    "                 trendline='ols')\n",
    "\n",
    "fig.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c652e",
   "metadata": {},
   "source": [
    "question9:\n",
    "Example Interpretation for Different Short Wait Limits\n",
    "Short Wait Limit = 62:\n",
    "If the p-value for the slope is small (e.g., less than 0.05), you can reject the null hypothesis, suggesting there is a statistically significant linear relationship between the waiting time and eruption duration for waiting times less than 62 minutes.\n",
    "If the p-value is large (greater than 0.05), you fail to reject the null hypothesis, meaning there is insufficient evidence for a linear relationship in this subset of data.\n",
    "Short Wait Limit = 64 or 66:\n",
    "You would repeat the process by changing short_wait_limit to 64 or 66, and observe whether the p-value for the slope changes significantly. If the relationship is still significant at these levels, the evidence for the linear association may hold; otherwise, the evidence for the linear relationship may weaken.\n",
    "Visualizing the Relationship\n",
    "The px.scatter function creates a scatter plot with the regression trendline, making it easier to visualize the relationship between the waiting time and eruption duration. If the trendline is a good fit to the data and the p-value is low, it suggests a strong linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "\n",
    "# Filter the long wait times data\n",
    "long_wait_limit = 71\n",
    "long_wait = old_faithful.waiting > long_wait_limit\n",
    "long_wait_data = old_faithful[long_wait]\n",
    "\n",
    "# Bootstrap sampling: generate bootstrap samples and fit Simple Linear Regression models\n",
    "n_bootstrap_samples = 1000\n",
    "bootstrapped_slope_coefficients = []\n",
    "\n",
    "for i in range(n_bootstrap_samples):\n",
    "    # Create bootstrap sample\n",
    "    bootstrap_sample = long_wait_data.sample(n=len(long_wait_data), replace=True)\n",
    "    \n",
    "    # Fit the model to the bootstrap sample\n",
    "    model = smf.ols('duration ~ waiting', data=bootstrap_sample).fit()\n",
    "    \n",
    "    # Collect the slope coefficient\n",
    "    bootstrapped_slope_coefficients.append(model.params[1])\n",
    "\n",
    "bootstrapped_slope_coefficients = np.array(bootstrapped_slope_coefficients)\n",
    "\n",
    "# Visualize the bootstrap sampling distribution of the slope coefficients\n",
    "fig = px.histogram(bootstrapped_slope_coefficients, nbins=30, \n",
    "                   title=\"Bootstrapped Sampling Distribution of Slope Coefficients\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a380d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data under the null hypothesis: no linear association\n",
    "simulated_slope_coefficients = []\n",
    "\n",
    "for i in range(n_bootstrap_samples):\n",
    "    # Create a simulated dataset where there is no linear association\n",
    "    old_faithful_simulation = old_faithful[long_wait].copy()\n",
    "    old_faithful_simulation['duration'] = 1.65 + 0 * old_faithful_simulation['waiting'] + stats.norm(loc=0, scale=0.37).rvs(size=len(long_wait_data))\n",
    "    \n",
    "    # Fit the model to the simulated data\n",
    "    model = smf.ols('duration ~ waiting', data=old_faithful_simulation).fit()\n",
    "    \n",
    "    # Collect the slope coefficient\n",
    "    simulated_slope_coefficients.append(model.params[1])\n",
    "\n",
    "simulated_slope_coefficients = np.array(simulated_slope_coefficients)\n",
    "\n",
    "# Visualize the simulated sampling distribution of slope coefficients under the null hypothesis\n",
    "fig = px.histogram(simulated_slope_coefficients, nbins=30, \n",
    "                   title=\"Simulated Sampling Distribution Under Null Hypothesis\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% bootstrapped confidence interval\n",
    "bootstrapped_ci = np.quantile(bootstrapped_slope_coefficients, [0.025, 0.975])\n",
    "print(f\"95% Bootstrapped Confidence Interval: {bootstrapped_ci}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the original data (long wait times)\n",
    "original_model = smf.ols('duration ~ waiting', data=long_wait_data).fit()\n",
    "observed_slope = original_model.params[1]\n",
    "\n",
    "# Calculate the simulated p-value based on the null hypothesis\n",
    "simulated_p_value = (np.abs(simulated_slope_coefficients) >= np.abs(observed_slope)).mean()\n",
    "print(f\"Simulated P-value: {simulated_p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model summary for comparison\n",
    "print(original_model.summary().tables[1])\n",
    "\n",
    "# Interpretation\n",
    "if simulated_p_value < 0.05:\n",
    "    print(\"We reject the null hypothesis: There is evidence for a linear relationship.\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis: There is no strong evidence for a linear relationship.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d033e15",
   "metadata": {},
   "source": [
    "question11:\n",
    "Step 1: Divide the Data into \"Short\" and \"Long\" Wait Times\n",
    "First, we will classify the old_faithful dataset into two groups:\n",
    "\n",
    "Short wait times: wait times < 68 minutes\n",
    "Long wait times: wait times >= 68 minutes\n",
    "We can do this by creating a new categorical variable called kind which indicates whether the wait time falls into the \"short\" or \"long\" category.\n",
    "\n",
    "Step 2: Use the Indicator Variable in the Model\n",
    "The new model specification uses C(kind, Treatment(reference=\"short\")), which is a way to treat kind as a categorical variable in the regression model. The Treatment(reference=\"short\") tells the model to treat the \"short\" category as the reference group, and the model will estimate the difference in duration between \"long\" wait times and the reference \"short\" wait times.\n",
    "Step 3: Big Picture Differences from the Previous Model Specifications\n",
    "Previous Models:\n",
    "smf.ols('duration ~ waiting', data=old_faithful): This model assumed a continuous relationship between wait time (waiting) and duration. The interpretation was based on the slope coefficient, which tells us how much the duration changes for every unit increase in wait time.\n",
    "smf.ols('duration ~ waiting', data=old_faithful[short_wait]): This was a subset of the data with short wait times only, testing the relationship between duration and waiting specifically for short wait times.\n",
    "smf.ols('duration ~ waiting', data=old_faithful[long_wait]): Similarly, this was a subset of the data with long wait times only, testing the relationship between duration and waiting for long wait times.\n",
    "New Model (with Indicator Variable):\n",
    "By including C(kind, Treatment(reference=\"short\")), we are explicitly treating the wait time length as a categorical variable with two levels (\"short\" and \"long\").\n",
    "The model will estimate a separate mean for the duration for each group (\"short\" and \"long\"), and the difference between the groups (in terms of their mean durations) will be the main focus, rather than the continuous relationship between waiting and duration.\n",
    "Step 4: Testing the Null Hypothesis of \"No Difference Between Groups\"\n",
    "We can test the null hypothesis that there is no difference in mean durations between the \"short\" and \"long\" wait time groups. This is done using the t-test for the difference in means, and the p-value from the model output will provide evidence against this null hypothesis.\n",
    "\n",
    "The null hypothesis is:\n",
    "\n",
    "H₀: \"There is no difference in mean duration between short and long wait times.\"\n",
    "H₁: \"There is a significant difference in mean duration between short and long wait times.\"\n",
    "Step 5: Display the Results and Visualize the Data\n",
    "Let's use the following code to fit the model, display the results, and visualize the difference in duration across the two wait time categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display\n",
    "\n",
    "# Create the new \"kind\" column based on wait time length\n",
    "old_faithful['kind'] = ['short' if wait < 68 else 'long' for wait in old_faithful['waiting']]\n",
    "\n",
    "# Fit the model with the categorical variable \"kind\"\n",
    "model = smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "display(model.summary().tables[1])\n",
    "\n",
    "# Boxplot of duration by kind (short vs long wait times)\n",
    "fig = px.box(old_faithful, x='kind', y='duration', \n",
    "             title='duration ~ kind', \n",
    "             category_orders={'kind': ['short', 'long']})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f8ff9",
   "metadata": {},
   "source": [
    "Explanation of the Results:\n",
    "Model Summary: The summary().tables[1] will show us the p-value for the kind[T.long] coefficient, which represents the difference in mean duration between the \"long\" and \"short\" groups. If the p-value is below a threshold (usually 0.05), we reject the null hypothesis, indicating there is a significant difference in mean durations between the groups.\n",
    "Boxplot: The boxplot visually illustrates the distributions of duration for the \"short\" and \"long\" wait time groups. The median and interquartile ranges will help us understand the differences in the duration values across the two categories.\n",
    "Step 6: Interpret the Evidence Against the Null Hypothesis\n",
    "If the p-value for the kind[T.long] coefficient is low (below 0.05), we would reject the null hypothesis and conclude that there is a significant difference in the duration of eruptions between the \"short\" and \"long\" wait times.\n",
    "If the p-value is high, we would fail to reject the null hypothesis, suggesting there is not enough evidence to support a significant difference in duration between the two groups.\n",
    "This model and analysis provide a more direct way to assess differences between the groups, compared to the previous continuous regression models.\n",
    "Summary:\n",
    "Key Steps:\n",
    "Data Classification:\n",
    "The dataset is split into two categories based on wait times:\n",
    "\"Short\" wait times: Wait times < 68 minutes.\n",
    "\"Long\" wait times: Wait times ≥ 68 minutes.\n",
    "New Model Specification:\n",
    "A new model is created using an indicator variable (kind) to represent the two groups (\"short\" and \"long\") instead of treating wait times as continuous.\n",
    "The model uses the formula duration ~ C(kind, Treatment(reference=\"short\")), where \"short\" is the reference group.\n",
    "This is a form of Analysis of Covariance (ANCOVA), estimating the difference in average eruption duration between \"short\" and \"long\" wait times.\n",
    "Hypothesis Testing:\n",
    "The null hypothesis (H₀): \"There is no difference in average eruption duration between the 'short' and 'long' wait times.\"\n",
    "The alternative hypothesis (H₁): \"There is a significant difference in eruption durations between the two groups.\"\n",
    "The p-value for the kind[T.long] coefficient is examined to test the null hypothesis.\n",
    "Visualization:\n",
    "A boxplot is created to visualize the distribution of eruption durations for the two groups (\"short\" and \"long\" wait times), showing their medians and interquartile ranges.\n",
    "Interpretation:\n",
    "If the p-value for the kind[T.long] coefficient is less than 0.05, we reject the null hypothesis, suggesting a significant difference in eruption durations between \"short\" and \"long\" wait times.\n",
    "If the p-value is greater than 0.05, we fail to reject the null hypothesis, indicating no significant difference between the groups.\n",
    "This approach allows for a clearer comparison of eruption durations between the two wait time groups, contrasting with previous models that focused on the continuous relationship between wait time and eruption duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444edf39",
   "metadata": {},
   "source": [
    "In the context of Simple Linear Regression, the assumption that the residuals (error terms) are normally distributed is important for valid statistical inference, such as hypothesis testing and confidence intervals. The histograms of residuals can help us assess the plausibility of this assumption.\n",
    "\n",
    "Key points for each model:\n",
    "Model 1: All Data using slope:\n",
    "The histogram of residuals from this model represents the data for all observations, with no differentiation between short and long wait times. If the residuals form a roughly symmetric bell-shaped distribution, this would support the assumption of normality. If the histogram is skewed or shows outliers, it would violate the assumption.\n",
    "Model 2: Short Wait Data:\n",
    "The histogram of residuals from this model represents a subset of the data with short wait times only. The key question is whether the residuals are symmetrically distributed around zero and resemble a bell-shaped curve. If the histogram deviates significantly from a normal shape (e.g., shows a heavy tail or skewness), it would suggest the model does not fit well with the assumption of normality for the residuals.\n",
    "Model 3: Long Wait Data:\n",
    "This histogram represents residuals from the long wait times subset of the data. As with the other models, we should check for symmetry and the bell-shaped nature of the distribution. If the residuals are not symmetrically distributed or exhibit non-normal patterns (e.g., heavy tails), this would indicate a violation of the normality assumption.\n",
    "Model 4: All Data using indicator:\n",
    "This model uses an indicator variable to differentiate between \"short\" and \"long\" wait times. If the histogram of residuals looks approximately normal (i.e., bell-shaped, with symmetry around zero), this would support the normality assumption. If the residuals show irregularities, such as multimodality or skewness, it would suggest a potential issue with the model’s fit.\n",
    "Interpretation of histograms:\n",
    "Model with Normal Distribution: If a histogram closely matches the overlaid normal distribution curve (black dotted line), it suggests that the residuals follow a normal distribution. This is typically ideal in Simple Linear Regression for valid hypothesis testing.\n",
    "Non-Normal Distribution in Histograms:\n",
    "Skewed residuals (i.e., one tail of the histogram is longer or more spread out than the other) or heavy tails (where extreme values occur more frequently than in a normal distribution) would suggest that the model might not be appropriate or that other assumptions (e.g., homoscedasticity, linearity) could be violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question13(A)\n",
    "# Observed mean difference\n",
    "observed_statistic = old_faithful.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "\n",
    "# Shuffling procedure\n",
    "n_iterations = 10000\n",
    "perm_stats = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    shuffled_labels = old_faithful.assign(kind_shuffled=old_faithful['kind'].sample(n=len(old_faithful), replace=False).values)\n",
    "    perm_stat = shuffled_labels.groupby('kind_shuffled')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "    perm_stats.append(perm_stat)\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = np.mean(np.abs(perm_stats) >= np.abs(observed_statistic))\n",
    "print(f'Permutation Test p-value: {p_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53556c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(B)\n",
    "# Initialize list for bootstrapped mean differences\n",
    "bootstrapped_mean_differences = []\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_iterations = 10000\n",
    "\n",
    "# Bootstrapping procedure\n",
    "for _ in range(n_iterations):\n",
    "    # Bootstrap samples within each group\n",
    "    boot_sample = old_faithful.groupby('kind').apply(lambda x: x.sample(n=len(x), replace=True)).reset_index(drop=True)\n",
    "    boot_stat = boot_sample.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "    bootstrapped_mean_differences.append(boot_stat)\n",
    "\n",
    "# 95% bootstrap confidence interval\n",
    "confidence_interval = np.quantile(bootstrapped_mean_differences, [0.025, 0.975])\n",
    "print(f'95% Bootstrap Confidence Interval: {confidence_interval}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30425c",
   "metadata": {},
   "source": [
    "(a) Explanation of the Sampling Approaches\n",
    "Permutation Test:\n",
    "The permutation test uses random shuffling of the group labels to simulate the null hypothesis of no difference between groups. By comparing the observed difference in means with the distribution of differences from shuffled datasets, the permutation test provides a p-value that quantifies how likely the observed difference is under the null hypothesis.\n",
    "This method does not require any assumptions about the distribution of the data (i.e., it is non-parametric) and works well even for small sample sizes.\n",
    "Bootstrap Confidence Interval:\n",
    "In bootstrapping, we repeatedly sample with replacement from the observed data to simulate what the sampling distribution of the mean difference would look like. Each bootstrap sample mimics drawing new samples from the population.\n",
    "By generating a large number of bootstrap samples and calculating the mean difference for each, we can construct a confidence interval for the difference between the groups. This method provides a measure of uncertainty around the mean difference estimate and is also non-parametric.\n",
    "(b) Comparison with Indicator Variable-based Model\n",
    "Similarity:\n",
    "Both the permutation test and the bootstrap method are non-parametric approaches that estimate the distribution of a statistic (mean difference) based on the observed data without making strong parametric assumptions about the underlying distribution.\n",
    "In contrast, the indicator variable-based model (e.g., smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful)) uses a statistical model to directly estimate the effect of the \"kind\" variable (short vs. long) on the outcome variable \"duration,\" assuming that the relationship between the variables is linear and that residuals follow certain statistical assumptions (normality, homoscedasticity, etc.).\n",
    "Difference:\n",
    "Permutation and Bootstrapping provide direct, data-driven ways of estimating the sampling distribution of the statistic (difference in means) without relying on any distributional assumptions. They are useful when we are unsure of the parametric assumptions or when sample sizes are small.\n",
    "The indicator variable-based model, on the other hand, is a parametric model that assumes a specific linear relationship between the predictor and the outcome variable. It estimates the mean difference using maximum likelihood estimation, but this relies on the assumption of normality in residuals and other regression assumptions.\n",
    "Summary:\n",
    " permutation and bootstrap methods are robust alternatives to traditional hypothesis tests and regression models, especially in cases where parametric assumptions might not hold or where the sample size is small. They allow for flexibility in estimating confidence intervals and p-values without relying on assumptions about the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456c333",
   "metadata": {},
   "source": [
    "question14:\n",
    "yes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
